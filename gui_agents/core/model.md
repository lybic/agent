# LLM Models

## 1. OpenAI

**æ”¯æŒçš„æ¨¡å‹ï¼š**

- `gpt-4o` - æœ€æ–°å¤šæ¨¡æ€æ¨¡å‹
- `gpt-4o-mini` - è½»é‡ç‰ˆ
- `gpt-4-turbo` - GPT-4 Turbo
- `gpt-4` - GPT-4 æ ‡å‡†ç‰ˆ
- `gpt-3.5-turbo` - GPT-3.5

## 2. Anthropic Claude

**æ”¯æŒçš„æ¨¡å‹ï¼š**

- `claude-opus-4` - Claude Opus 4
- `claude-sonnet-4` - Claude Sonnet 4
- `claude-3-5-sonnet` - Claude 3.5 Sonnet
- `claude-3-5-haiku` - Claude 3.5 Haiku
- `claude-3-opus` - Claude 3 Opus
- `claude-3-sonnet` - Claude 3 Sonnet
- `claude-3-haiku` - Claude 3 Haiku

### 3. é˜¿é‡Œäº‘ Qwen

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `qwen-max` - é€šä¹‰åƒé—®æœ€å¤§æ¨¡å‹
- `qwen-plus` - é€šä¹‰åƒé—®å¢å¼ºç‰ˆ
- `qwen-turbo` - é€šä¹‰åƒé—®æ ‡å‡†ç‰ˆ
- `qwen2.5-72b-instruct` - Qwen2.5 72B
- `qwen2.5-32b-instruct` - Qwen2.5 32B
- `qwen2.5-14b-instruct` - Qwen2.5 14B
- `qwen2.5-7b-instruct` - Qwen2.5 7B

```python
from engine import LMMEngineQwen

# åˆå§‹åŒ–
qwen = LMMEngineQwen(
    model="qwen-max",
    enable_thinking=False  # æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
)

# ç”Ÿæˆæ–‡æœ¬
messages = [{"role": "user", "content": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}]
response = qwen.generate(messages, temperature=0.8)
```

### 4. å­—èŠ‚è·³åŠ¨ Doubao

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `Doubao-1.5-thinking-vision-pro` - è±†åŒ…æ¨¡å‹ï¼ˆéœ€è¦æ›¿æ¢ä¸ºå®é™…æ¨¡å‹ï¼‰
- å…·ä½“æ¨¡å‹åç§°éœ€è¦åœ¨ç«å±±å¼•æ“æ§åˆ¶å°æŸ¥çœ‹

```python
from engine import LMMEngineDoubao

# åˆå§‹åŒ–
doubao = LMMEngineDoubao(model="Doubao-1.5-thinking-vision-pro")

# ç”Ÿæˆæ–‡æœ¬
messages = [{"role": "user", "content": "ä»‹ç»äººå·¥æ™ºèƒ½"}]
response = doubao.generate(messages)
```

### 5. Google Gemini

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `gemini-2.5-pro` - Gemini 2.5 Pro
- `gemini-2.5-flash` - Gemini 2.5 Flash


```python
from engine import LMMEngineGemini

# åˆå§‹åŒ–
gemini = LMMEngineGemini(model="gemini-2.5-pro")

# ç”Ÿæˆæ–‡æœ¬
messages = [{"role": "user", "content": "è§£é‡Šç›¸å¯¹è®º"}]
response = gemini.generate(messages)
```

### 6. DeepSeek

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `deepseek-chat` - DeepSeek Chat
- `deepseek-reasoner` - DeepSeek Reasoner

```python
from engine import LMMEngineDeepSeek

# åˆå§‹åŒ–
deepseek = LMMEngineDeepSeek(model="deepseek-chat")

# ç”Ÿæˆæ–‡æœ¬
messages = [{"role": "user", "content": "ç¼–å†™Pythonå¿«é€Ÿæ’åº"}]
response = deepseek.generate(messages)
```

### 7. æ™ºè°± GLM

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `glm-4-plus` - GLM-4 Plus
- `glm-4-0520` - GLM-4 
- `glm-4-air` - GLM-4 Air
- `glm-4-airx` - GLM-4 AirX
- `glm-4-flash` - GLM-4 Flash

```python
from engine import LMMEngineZhipu

# åˆå§‹åŒ–
zhipu = LMMEngineZhipu(model="glm-4-plus")

# ç”Ÿæˆæ–‡æœ¬
messages = [{"role": "user", "content": "åˆ†æç»æµå½¢åŠ¿"}]
response = zhipu.generate(messages)
```

### 8. Groq

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `llama-3.1-405b-reasoning` - Llama 3.1 405B
- `llama-3.1-70b-versatile` - Llama 3.1 70B
- `llama-3.1-8b-instant` - Llama 3.1 8B
- `mixtral-8x7b-32768` - Mixtral 8x7B
- `gemma2-9b-it` - Gemma 2 9B

```python
from engine import LMMEngineGroq

# åˆå§‹åŒ–
groq = LMMEngineGroq(model="llama-3.1-70b-versatile")

# ç”Ÿæˆæ–‡æœ¬
messages = [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}]
response = groq.generate(messages)
```

### 9. AWS Bedrock

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `claude-opus-4` - Claude Opus 4
- `claude-sonnet-4` - Claude Sonnet 4
- `claude-3-5-sonnet` - Claude 3.5 Sonnet
- `claude-3-5-haiku` - Claude 3.5 Haiku
- `claude-3-opus` - Claude 3 Opus
- `claude-3-sonnet` - Claude 3 Sonnet
- `claude-3-haiku` - Claude 3 Haiku

```python
from engine import LMMEngineAWSBedrock

# åˆå§‹åŒ–
bedrock = LMMEngineAWSBedrock(model="claude-3-5-sonnet")

# ç”Ÿæˆæ–‡æœ¬
messages = [{"role": "user", "content": "ä»‹ç»äº‘è®¡ç®—"}]
response = bedrock.generate(messages)
```

## ğŸ”¤ Embedding å¼•æ“

### 1. OpenAI Embeddings

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `text-embedding-3-large` - æœ€å¤§æ¨¡å‹ (3072ç»´)
- `text-embedding-3-small` - å°æ¨¡å‹ (1536ç»´)
- `text-embedding-ada-002` - ç»å…¸æ¨¡å‹ (1536ç»´)

```python
from engine import OpenAIEmbeddingEngine

# åˆå§‹åŒ–
embedder = OpenAIEmbeddingEngine(embedding_model="text-embedding-3-small")

# è·å–å‘é‡
text = "è¿™æ˜¯ä¸€æ®µéœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬"
embeddings = embedder.get_embeddings(text)
print(f"å‘é‡ç»´åº¦: {embeddings.shape}")  # (1, 1536)
```

### 2. Google Gemini Embeddings

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `text-embedding-004` - Gemini æœ€æ–°åµŒå…¥æ¨¡å‹

```python
from engine import GeminiEmbeddingEngine

# åˆå§‹åŒ–
gemini_embedder = GeminiEmbeddingEngine(embedding_model="text-embedding-004")

# è·å–å‘é‡
text = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯"
embeddings = gemini_embedder.get_embeddings(text)
```

### 3. é˜¿é‡Œäº‘ DashScope Embeddings

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `text-embedding-v4` - æœ€æ–°ç‰ˆæœ¬
- `text-embedding-v3` - æ ‡å‡†ç‰ˆæœ¬
- `text-embedding-v2` - ç»å…¸ç‰ˆæœ¬

```python
from engine import DashScopeEmbeddingEngine

# åˆå§‹åŒ–
dashscope_embedder = DashScopeEmbeddingEngine(
    embedding_model="text-embedding-v4",
    dimensions=1024  # å¯é€‰ï¼š512, 768, 1024, 1536
)

# è·å–å‘é‡
text = "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†"
embeddings = dashscope_embedder.get_embeddings(text)
```

### 4. å­—èŠ‚è·³åŠ¨ Doubao Embeddings

**æ”¯æŒçš„æ¨¡å‹ï¼š**

doubao-embedding-vision-250615 ï¼šinput æ”¯æŒä¸é™æ•°é‡çš„ æ–‡æœ¬ä¿¡æ¯ã€å›¾ç‰‡ä¿¡æ¯å’Œ è§†é¢‘ä¿¡æ¯æ··æ’è¾“å…¥ã€‚ä¼ å…¥çš„ä¿¡æ¯ä½œä¸º1ä¸ªæ•´ä½“è¿›è¡Œå‘é‡åŒ–ã€‚

doubao-embedding-vision-250328/doubao-embedding-vision-241215 : input å½“å‰ä»…æ”¯æŒ3ç§ç»„åˆï¼Œ 1æ®µæ–‡æœ¬ä¿¡æ¯ã€1æ®µå›¾ç‰‡ä¿¡æ¯ã€ 1æ®µå›¾ç‰‡ä¿¡æ¯+1æ®µæ–‡æœ¬ä¿¡æ¯ã€‚


```python
from engine import DoubaoEmbeddingEngine

# åˆå§‹åŒ–
doubao_embedder = DoubaoEmbeddingEngine(embedding_model="doubao-embedding-vision-250615")

# è·å–å‘é‡
text = "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å‘å±•"
embeddings = doubao_embedder.get_embeddings(text)
```

### 5. Jina AI Embeddings

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `jina-embeddings-v4` - æœ€æ–°ç‰ˆæœ¬
- `jina-embeddings-v3` - ç¬¬ä¸‰ä»£
- `jina-clip-v2` - å¤šæ¨¡æ€åµŒå…¥

```python
from engine import JinaEmbeddingEngine

# åˆå§‹åŒ–
jina_embedder = JinaEmbeddingEngine(
    embedding_model="jina-embeddings-v4",
    task="retrieval.query"  # "retrieval.passage", "text-matching"
)

# è·å–å‘é‡
text = "ä¿¡æ¯æ£€ç´¢æ˜¯è®¡ç®—æœºç§‘å­¦é‡è¦é¢†åŸŸ"
embeddings = jina_embedder.get_embeddings(text)
```

## ğŸ” æœç´¢å¼•æ“

### 1. Bocha AI Search

æ™ºèƒ½æœç´¢å¼•æ“ï¼Œè¿”å›AIåˆ†æåçš„ç­”æ¡ˆå’Œå‚è€ƒæ¥æºã€‚

```python
from engine import BochaAISearchEngine

# åˆå§‹åŒ–
bocha_search = BochaAISearchEngine()

# åŸºæœ¬æœç´¢
result = bocha_search.search(
    query="è¥¿ç“œçš„åŠŸæ•ˆä¸ä½œç”¨",
    freshness="noLimit",  # "day", "week", "month", "year", "noLimit"
    answer=True,          # æ˜¯å¦è¿”å›AIç­”æ¡ˆ
    stream=False          # æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
)

# å¿«æ·æ–¹æ³•
answer = bocha_search.get_answer("è¥¿ç“œçš„åŠŸæ•ˆä¸ä½œç”¨")
sources = bocha_search.get_sources("è¥¿ç“œçš„åŠŸæ•ˆä¸ä½œç”¨")
follow_ups = bocha_search.get_follow_up_questions("è¥¿ç“œçš„åŠŸæ•ˆä¸ä½œç”¨")

# æµå¼æœç´¢
for chunk in bocha_search.search("å¤©ç©ºä¸ºä»€ä¹ˆæ˜¯è“è‰²", stream=True):
    print(chunk)
```

### 2. Exa Research

æ·±åº¦ç ”ç©¶å¼•æ“ï¼Œé€‚åˆå¤æ‚ä¸»é¢˜çš„å­¦æœ¯ç ”ç©¶ã€‚

**æ”¯æŒçš„æ¨¡å‹ï¼š**
- `exa-research` - ä¸“ä¸šç ”ç©¶æ¨¡å‹
